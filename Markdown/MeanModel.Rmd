---
title: "Use Mean"
author: "AlexandeR Noll"
date: "`r Sys.Date()"
output: html_document
---

```{r, include = FALSE}
library(dplyr)
library(tidyr)
library(caret)
library(quantreg)

ComputeMean <- function(df){
    return_variables <-
        paste0("Ret_", 2:120)
    
    df %>%
        select(Id, one_of(return_variables)) %>%
        gather(time, returns, 2:120) %>%
        select(-time) %>%
        group_by(Id) %>%
        summarise(mean_return = mean(returns),
                  sd_return = sd(returns),
                  median_return = median(returns)) %>%
        inner_join(df, by = "Id") %>%
        return
}

MAE <- function(data, lev = NULL, model = NULL){
    # Returns mean absolute error; to be used in caret package for choosing
    # optimization goal; 
    #
    # Args: data: data frame having two variables obs and pred
    #       lev: unimportant for regression problems
    #       model: unimportant for regression problems
    #
    # Returns: labeled vector with mean absolute error
    mae <- function(obs, pred){
        mean(abs(obs-pred))
    }
    return(c("MAE" = mae(data$obs, data$pred)))
}
```

In this document, we play around a little with different models. This is more for exploring the data than building a final model.

# Data preparation

```{r}
set.seed(21)
train <- 
    readr::read_csv("~/Desktop/Data_Science_Scale/Winton/Winton/Data/train.csv")
train_ind <- createDataPartition(train$Ret_121, list = FALSE, p = 0.8)
train_train <- train[train_ind, ]
train_cv <- train[-train_ind, ]
```

We impute missing data by median:

```{r}
impute_model <-
    preProcess(train_train, method = "medianImpute")
train_train %<>% predict(impute_model, .) %>% ComputeMean
train_cv %<>% predict(impute_model, .) %>% ComputeMean
```

# Fit models

It is extremely important to choose the right optimization goal. In previous attempts, the least squares optimization was chosen (which was obviously wrong) and so the optimization performed worse on the training set itself.

## Predict next time step

Here, our goal is very humble: just try to predict the next time step.

### Use autocorrelation

As discussed in the document *Simplest*, there is autocorrelation at lag 1. We try to use this to predict the next time step *Ret_121*.

```{r}
ret_121_model <- rq(Ret_121 ~ Ret_120, 
                    tau = 0.5, 
                    data = train_train
                    )
```

Let us evaluate the trained model against both the training set and the cross validation set to check that the results look a bit better now: 

```{r}
# Training set
# Error of model predictions
MAE(data.frame(obs = train_train$Ret_121, 
               pred = predict(ret_121_model, newdata = train_train)
               )
    )

# Error of zero prediciton
MAE(data.frame(obs = train_train$Ret_121,
               pred = rep(0, nrow(train_train))
               )
    )

# Cross validation set
# Error of model predictions
MAE(data.frame(obs = train_cv$Ret_121, 
               pred = predict(ret_121_model, newdata = train_cv)
               )
    )

# Error of zero predicitons
MAE(data.frame(obs = train_cv$Ret_121,
               pred = rep(0, nrow(train_train))
               )
    )
```

So, we "outperform" the zero prediction at least a little bit.

### Add more predictors

In this step, we add some more predictors, in particular the ones generated in the data preparation step, i.e. the mean return, the standard deviation of the return and the median return up to that time step:

```{r}
ret_121_model <- rq(Ret_121 ~ Ret_120 + mean_return + median_return + sd_return, 
                    tau = 0.5, 
                    data = train_train
                    )
```

Evaluate:

```{r}
# Training set
# Error of model predictions
MAE(data.frame(obs = train_train$Ret_121, 
               pred = predict(ret_121_model, newdata = train_train)
               )
    )

# Error of zero prediciton
MAE(data.frame(obs = train_train$Ret_121,
               pred = rep(0, nrow(train_train))
               )
    )

# Cross validation set
# Error of model predictions
MAE(data.frame(obs = train_cv$Ret_121, 
               pred = predict(ret_121_model, newdata = train_cv)
               )
    )

# Error of zero predicitons
MAE(data.frame(obs = train_cv$Ret_121,
               pred = rep(0, nrow(train_train))
               )
    )
```

We see that on the training set, we significantly outperform the zero predicition model and also on the cross validation set, we perform slightly better, but there is some overfitting already.

### Use all features

Now, we try to use all features (*Feature_1* - *Feature_25*):

```{r}
ret_121_model <- rq(Ret_121 ~ ., 
                    tau = 0.5, 
                    data = train_train %>% select(Ret_121,
                                                  Ret_PlusOne,
                                                  Ret_PlusTwo,
                                                  Ret_120, 
                                                  mean_return, 
                                                  median_return,
                                                  sd_return,
                                                  starts_with("Feat")
                                                 )
                    )

```

Evaluate:

```{r}
# Training set
# Error of model predictions
MAE(data.frame(obs = train_train$Ret_121, 
               pred = predict(ret_121_model, newdata = train_train)
               )
    )

# Error of zero prediciton
MAE(data.frame(obs = train_train$Ret_121,
               pred = rep(0, nrow(train_train))
               )
    )

# Cross validation set
# Error of model predictions
MAE(data.frame(obs = train_cv$Ret_121, 
               pred = predict(ret_121_model, newdata = train_cv)
               )
    )

# Error of zero predicitons
MAE(data.frame(obs = train_cv$Ret_121,
               pred = rep(0, nrow(train_train))
               )
    )
```

This still gives a slight improvement and is the best model up to this point.